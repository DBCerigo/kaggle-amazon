{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../')\n",
    "import ama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vgg = ama.vgg.Vgg\n",
    "TrainBatch = ama.trainbatch.TrainBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path= '../data/'\n",
    "cv_version = 1\n",
    "cv_version = str(cv_version)\n",
    "batch_size = 128\n",
    "img_size = (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg = Vgg(input_shape=(3,)+img_size, dropout=0)\n",
    "vgg.load_weights('../data/weights/atc_1.hk', loaded_dropout=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "valgen = TrainBatch(path+'val-jpg'+cv_version+'/', path+'train_v2.csv', batch_size=batch_size, img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34479 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "gen = ImageDataGenerator(rotation_range=5,\n",
    "        horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "traingen = TrainBatch(path+'train-jpg'+cv_version+'/', path+'train_v2.csv', batch_size=batch_size, img_size=img_size,\n",
    "                      imagegen=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from ama.persistenthistory import PersistentHistory\n",
    "\n",
    "earlystop = EarlyStopping(patience=4)\n",
    "history = PersistentHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_saver(fp):\n",
    "    return ModelCheckpoint(filepath, verbose=1, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = '../data/weights/atc'+cv_version+'_less_dropout_best_lr1.hk'\n",
    "saver = get_saver(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9622Epoch 00000: val_loss improved from inf to 0.10222, saving model to ../data/weights/atc1_less_dropout_best_lr1.hk\n",
      "34479/34479 [==============================] - 396s - loss: 0.0963 - acc: 0.9622 - val_loss: 0.1022 - val_acc: 0.9608\n",
      "Epoch 2/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9628Epoch 00001: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0946 - acc: 0.9628 - val_loss: 0.1037 - val_acc: 0.9610\n",
      "Epoch 3/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9637Epoch 00002: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0923 - acc: 0.9638 - val_loss: 0.1042 - val_acc: 0.9612\n",
      "Epoch 4/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9641Epoch 00003: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0908 - acc: 0.9641 - val_loss: 0.1030 - val_acc: 0.9613\n",
      "Epoch 5/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9648Epoch 00004: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0889 - acc: 0.9648 - val_loss: 0.1049 - val_acc: 0.9607\n",
      "Epoch 6/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9656Epoch 00005: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0870 - acc: 0.9656 - val_loss: 0.1054 - val_acc: 0.9602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68164051d0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=20,\n",
    "                        validation_data=valgen, nb_val_samples=valgen.nb_sample,\n",
    "                        callbacks=[history, saver, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like that the model is training and overfitting a bit. I think this means we need more augmentation basically. I'm also thinking about removing / extending the patience on the early stopping, though. Let's first try a lower LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9677Epoch 00000: val_loss improved from 0.10222 to 0.09852, saving model to ../data/weights/atc1_less_dropout_best_lr1.hk\n",
      "34479/34479 [==============================] - 182s - loss: 0.0815 - acc: 0.9677 - val_loss: 0.0985 - val_acc: 0.9620\n",
      "Epoch 2/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9685Epoch 00001: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0794 - acc: 0.9685 - val_loss: 0.0997 - val_acc: 0.9621\n",
      "Epoch 3/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9689Epoch 00002: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0785 - acc: 0.9689 - val_loss: 0.0998 - val_acc: 0.9623\n",
      "Epoch 4/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9693Epoch 00003: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0778 - acc: 0.9693 - val_loss: 0.1004 - val_acc: 0.9620\n",
      "Epoch 5/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9694Epoch 00004: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0771 - acc: 0.9694 - val_loss: 0.1000 - val_acc: 0.9622\n",
      "Epoch 6/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9696Epoch 00005: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0765 - acc: 0.9697 - val_loss: 0.0996 - val_acc: 0.9622\n",
      "Epoch 7/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9698Epoch 00006: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0761 - acc: 0.9698 - val_loss: 0.1004 - val_acc: 0.9619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6809807fd0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.compile(optimizer=Adam(lr=0.0001),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "earlystop = EarlyStopping(patience=5)\n",
    "vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=10,\n",
    "                        validation_data=valgen, nb_val_samples=valgen.nb_sample,\n",
    "                        callbacks=[history, saver, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we crept below 0.1, which is good. The real baddy boys @ 0.93 on the LB are pushing 0.08 loss though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights(filepath)\n",
    "filepath = '../data/weights/atc'+cv_version+'_less_dropout_best_lr3.hk'\n",
    "saver = get_saver(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9688Epoch 00000: val_loss improved from inf to 0.09763, saving model to ../data/weights/atc1_less_dropout_best_lr3.hk\n",
      "34479/34479 [==============================] - 179s - loss: 0.0791 - acc: 0.9688 - val_loss: 0.0976 - val_acc: 0.9623\n",
      "Epoch 2/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9686Epoch 00001: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0795 - acc: 0.9686 - val_loss: 0.0976 - val_acc: 0.9622\n",
      "Epoch 3/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9689Epoch 00002: val_loss improved from 0.09763 to 0.09745, saving model to ../data/weights/atc1_less_dropout_best_lr3.hk\n",
      "34479/34479 [==============================] - 183s - loss: 0.0789 - acc: 0.9689 - val_loss: 0.0975 - val_acc: 0.9623\n",
      "Epoch 4/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9689Epoch 00003: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0785 - acc: 0.9689 - val_loss: 0.0982 - val_acc: 0.9622\n",
      "Epoch 5/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9689Epoch 00004: val_loss improved from 0.09745 to 0.09743, saving model to ../data/weights/atc1_less_dropout_best_lr3.hk\n",
      "34479/34479 [==============================] - 182s - loss: 0.0784 - acc: 0.9689 - val_loss: 0.0974 - val_acc: 0.9622\n",
      "Epoch 6/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9688Epoch 00005: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0786 - acc: 0.9688 - val_loss: 0.0988 - val_acc: 0.9623\n",
      "Epoch 7/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9691Epoch 00006: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0781 - acc: 0.9691 - val_loss: 0.0992 - val_acc: 0.9623\n",
      "Epoch 8/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9691Epoch 00007: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0784 - acc: 0.9690 - val_loss: 0.0993 - val_acc: 0.9623\n",
      "Epoch 9/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9691Epoch 00008: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0780 - acc: 0.9691 - val_loss: 0.0993 - val_acc: 0.9622\n",
      "Epoch 10/10\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9691Epoch 00009: val_loss did not improve\n",
      "34479/34479 [==============================] - 182s - loss: 0.0778 - acc: 0.9691 - val_loss: 0.0992 - val_acc: 0.9622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f682a7ebad0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.compile(optimizer=Adam(lr=0.00001),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=10,\n",
    "                        validation_data=valgen, nb_val_samples=valgen.nb_sample,\n",
    "                        callbacks=[history, saver, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're overfitting now. Let's try more data augmentation, and after that we can crank up the dropout a bit if necessary. I want to try to avoid adding too much \"noise\" to the data (ie through augmentations that mangle the data too much - think about whats happening to the image - or through too much dropout) as this will stop us learning enough to get our val loss down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights(filepath)\n",
    "filepath = '../data/weights/atc'+cv_version+'_less_dropout_more_aug_best_lr3.hk'\n",
    "saver = get_saver(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34479 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "gen = ImageDataGenerator(rotation_range=10, shear_range=10,\n",
    "        horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "traingen = TrainBatch(path+'train-jpg'+cv_version+'/', path+'train_v2.csv', batch_size=batch_size, img_size=img_size,\n",
    "                      imagegen=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9530Epoch 00000: val_loss improved from inf to 0.10578, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 181s - loss: 0.1254 - acc: 0.9530 - val_loss: 0.1058 - val_acc: 0.9595\n",
      "Epoch 2/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9550Epoch 00001: val_loss improved from 0.10578 to 0.10352, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 184s - loss: 0.1200 - acc: 0.9550 - val_loss: 0.1035 - val_acc: 0.9604\n",
      "Epoch 3/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9547Epoch 00002: val_loss improved from 0.10352 to 0.10314, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 183s - loss: 0.1199 - acc: 0.9547 - val_loss: 0.1031 - val_acc: 0.9608\n",
      "Epoch 4/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9551Epoch 00003: val_loss improved from 0.10314 to 0.10244, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 185s - loss: 0.1190 - acc: 0.9551 - val_loss: 0.1024 - val_acc: 0.9610\n",
      "Epoch 5/10\n",
      "11392/34479 [========>.....................] - ETA: 111s - loss: 0.1206 - acc: 0.9546"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-861621ebce87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=10,\n\u001b[1;32m      3\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         callbacks=[history, saver, earlystop])\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1442\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg.model.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=10,\n",
    "                        validation_data=valgen, nb_val_samples=valgen.nb_sample,\n",
    "                        callbacks=[history, saver, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it really didn't like that - was it cause I cranked the LR up too high or something? Let's try again the same setup but with LR/10 (after reloading the weights from before). Otherwise, we can get rid of some of the augmentation since it was underfitting a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights('../data/weights/atc'+cv_version+'_less_dropout_best_lr3.hk')\n",
    "filepath = '../data/weights/atc'+cv_version+'_less_dropout_more_aug_best_lr3.hk'\n",
    "saver = get_saver(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, I'm just gonna check this is loading the weights right - aka that the val loss should be 0.0974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09742651663223903, 0.96219607512156169]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.evaluate_generator(valgen, valgen.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, let's crack on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34479 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "gen = ImageDataGenerator(rotation_range=10, shear_range=10,\n",
    "        horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "traingen = TrainBatch(path+'train-jpg'+cv_version+'/', path+'train_v2.csv', batch_size=batch_size, img_size=img_size,\n",
    "                      imagegen=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9528Epoch 00000: val_loss improved from inf to 0.10557, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 182s - loss: 0.1292 - acc: 0.9528 - val_loss: 0.1056 - val_acc: 0.9598\n",
      "Epoch 2/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9547Epoch 00001: val_loss improved from 0.10557 to 0.10196, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 182s - loss: 0.1217 - acc: 0.9547 - val_loss: 0.1020 - val_acc: 0.9610\n",
      "Epoch 3/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9556Epoch 00002: val_loss improved from 0.10196 to 0.10174, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 185s - loss: 0.1192 - acc: 0.9556 - val_loss: 0.1017 - val_acc: 0.9613\n",
      "Epoch 4/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9557Epoch 00003: val_loss improved from 0.10174 to 0.09944, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 182s - loss: 0.1181 - acc: 0.9557 - val_loss: 0.0994 - val_acc: 0.9614\n",
      "Epoch 5/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9566Epoch 00004: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.1164 - acc: 0.9566 - val_loss: 0.0997 - val_acc: 0.9616\n",
      "Epoch 6/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9564Epoch 00005: val_loss did not improve\n",
      "34479/34479 [==============================] - 182s - loss: 0.1166 - acc: 0.9564 - val_loss: 0.0995 - val_acc: 0.9617\n",
      "Epoch 7/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9568Epoch 00006: val_loss improved from 0.09944 to 0.09937, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 183s - loss: 0.1157 - acc: 0.9568 - val_loss: 0.0994 - val_acc: 0.9614\n",
      "Epoch 8/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9570Epoch 00007: val_loss improved from 0.09937 to 0.09891, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 182s - loss: 0.1147 - acc: 0.9570 - val_loss: 0.0989 - val_acc: 0.9614\n",
      "Epoch 9/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9568Epoch 00008: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.1142 - acc: 0.9568 - val_loss: 0.0990 - val_acc: 0.9617\n",
      "Epoch 10/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9569Epoch 00009: val_loss improved from 0.09891 to 0.09857, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 182s - loss: 0.1146 - acc: 0.9569 - val_loss: 0.0986 - val_acc: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6809807e50>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.compile(optimizer=Adam(lr=0.0001),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=10,\n",
    "                        validation_data=valgen, nb_val_samples=valgen.nb_sample,\n",
    "                        callbacks=[history, saver, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I'm not convinced. We're underfitting again but I think maybe the noise is too much. I'm gonna lower the aug down to 5 deg on rotation & shear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights('../data/weights/atc'+cv_version+'_less_dropout_best_lr3.hk')\n",
    "filepath = '../data/weights/atc'+cv_version+'_less_dropout_more_aug_best_lr3.hk'\n",
    "saver = get_saver(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34479 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "gen = ImageDataGenerator(rotation_range=5, shear_range=5,\n",
    "        horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "traingen = TrainBatch(path+'train-jpg'+cv_version+'/', path+'train_v2.csv', batch_size=batch_size, img_size=img_size,\n",
    "                      imagegen=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9516Epoch 00000: val_loss improved from inf to 0.10732, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 182s - loss: 0.1331 - acc: 0.9516 - val_loss: 0.1073 - val_acc: 0.9598\n",
      "Epoch 2/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9535Epoch 00001: val_loss improved from 0.10732 to 0.10263, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 181s - loss: 0.1253 - acc: 0.9535 - val_loss: 0.1026 - val_acc: 0.9610\n",
      "Epoch 3/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9545Epoch 00002: val_loss improved from 0.10263 to 0.10127, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 181s - loss: 0.1221 - acc: 0.9545 - val_loss: 0.1013 - val_acc: 0.9610\n",
      "Epoch 4/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9550Epoch 00003: val_loss improved from 0.10127 to 0.10048, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 183s - loss: 0.1206 - acc: 0.9550 - val_loss: 0.1005 - val_acc: 0.9610\n",
      "Epoch 5/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9552Epoch 00004: val_loss improved from 0.10048 to 0.10041, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 183s - loss: 0.1203 - acc: 0.9551 - val_loss: 0.1004 - val_acc: 0.9612\n",
      "Epoch 6/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9556Epoch 00005: val_loss improved from 0.10041 to 0.09976, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 183s - loss: 0.1190 - acc: 0.9556 - val_loss: 0.0998 - val_acc: 0.9615\n",
      "Epoch 7/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9554Epoch 00006: val_loss improved from 0.09976 to 0.09947, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 185s - loss: 0.1189 - acc: 0.9554 - val_loss: 0.0995 - val_acc: 0.9615\n",
      "Epoch 8/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9560Epoch 00007: val_loss improved from 0.09947 to 0.09879, saving model to ../data/weights/atc1_less_dropout_more_aug_best_lr3.hk\n",
      "34479/34479 [==============================] - 187s - loss: 0.1177 - acc: 0.9560 - val_loss: 0.0988 - val_acc: 0.9616\n",
      "Epoch 9/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9558Epoch 00008: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1182 - acc: 0.9558 - val_loss: 0.0990 - val_acc: 0.9616\n",
      "Epoch 10/10\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9561Epoch 00009: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.1176 - acc: 0.9561 - val_loss: 0.0990 - val_acc: 0.9617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f686d4bca50>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.compile(optimizer=Adam(lr=0.0001),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=10,\n",
    "                        validation_data=valgen, nb_val_samples=valgen.nb_sample,\n",
    "                        callbacks=[history, saver, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So slow to get down. I'm gonna try higher for a long time, do some other jobs, and come back to check it out later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9538Epoch 00000: val_loss did not improve\n",
      "34479/34479 [==============================] - 179s - loss: 0.1229 - acc: 0.9538 - val_loss: 0.1044 - val_acc: 0.9594\n",
      "Epoch 2/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9538Epoch 00001: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.1226 - acc: 0.9538 - val_loss: 0.1038 - val_acc: 0.9595\n",
      "Epoch 3/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9541Epoch 00002: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.1215 - acc: 0.9541 - val_loss: 0.1029 - val_acc: 0.9595\n",
      "Epoch 4/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9544Epoch 00003: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1211 - acc: 0.9543 - val_loss: 0.1008 - val_acc: 0.9607\n",
      "Epoch 5/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9545Epoch 00004: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1202 - acc: 0.9545 - val_loss: 0.1032 - val_acc: 0.9599\n",
      "Epoch 6/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9552Epoch 00005: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1186 - acc: 0.9552 - val_loss: 0.1018 - val_acc: 0.9606\n",
      "Epoch 7/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9548Epoch 00006: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.1193 - acc: 0.9547 - val_loss: 0.1031 - val_acc: 0.9593\n",
      "Epoch 8/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9548Epoch 00007: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1193 - acc: 0.9548 - val_loss: 0.1024 - val_acc: 0.9604\n",
      "Epoch 9/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9553Epoch 00008: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1184 - acc: 0.9553 - val_loss: 0.1027 - val_acc: 0.9597\n",
      "Epoch 10/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9557Epoch 00009: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1171 - acc: 0.9557 - val_loss: 0.1049 - val_acc: 0.9588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6809178f90>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=30,\n",
    "                        validation_data=valgen, nb_val_samples=valgen.nb_sample,\n",
    "                        callbacks=[history, saver, earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9556Epoch 00000: val_loss did not improve\n",
      "34479/34479 [==============================] - 179s - loss: 0.1180 - acc: 0.9556 - val_loss: 0.1025 - val_acc: 0.9601\n",
      "Epoch 2/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9552Epoch 00001: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.1193 - acc: 0.9551 - val_loss: 0.2629 - val_acc: 0.9189\n",
      "Epoch 3/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9504Epoch 00002: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.1332 - acc: 0.9504 - val_loss: 0.1073 - val_acc: 0.9595\n",
      "Epoch 4/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9546Epoch 00003: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1210 - acc: 0.9546 - val_loss: 0.1033 - val_acc: 0.9600\n",
      "Epoch 5/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9556Epoch 00004: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1179 - acc: 0.9557 - val_loss: 0.1018 - val_acc: 0.9605\n",
      "Epoch 6/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9561Epoch 00005: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1162 - acc: 0.9561 - val_loss: 0.1011 - val_acc: 0.9606\n",
      "Epoch 7/30\n",
      "34351/34479 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9565Epoch 00006: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.1152 - acc: 0.9565 - val_loss: 0.1026 - val_acc: 0.9603\n",
      "Epoch 8/30\n",
      " 6016/34479 [====>.........................] - ETA: 136s - loss: 0.1170 - acc: 0.9555"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-3803d797dc25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=30,\n\u001b[1;32m      3\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         callbacks=[history, saver, earlystop])\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1442\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg.model.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=30,\n",
    "                        validation_data=valgen, nb_val_samples=valgen.nb_sample,\n",
    "                        callbacks=[history, saver, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really not convinced by this. Gonna try increasing dropout a bit and training on the old aug techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg(input_shape=(3,)+img_size, dropout=0.1)\n",
    "vgg.load_weights('../data/weights/atc'+cv_version+'_less_dropout_best_lr3.hk', loaded_dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '../data/weights/atc'+cv_version+'_slightly_less_dropout_best_lr3.hk'\n",
    "saver = get_saver(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "valgen = TrainBatch(path+'val-jpg'+cv_version+'/', path+'train_v2.csv', batch_size=batch_size, img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34479 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "gen = ImageDataGenerator(rotation_range=5,\n",
    "        horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "traingen = TrainBatch(path+'train-jpg'+cv_version+'/', path+'train_v2.csv', batch_size=batch_size, img_size=img_size,\n",
    "                      imagegen=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9658Epoch 00000: val_loss improved from inf to 0.10741, saving model to ../data/weights/atc1_slightly_less_dropout_best_lr3.hk\n",
      "34479/34479 [==============================] - 179s - loss: 0.0863 - acc: 0.9658 - val_loss: 0.1074 - val_acc: 0.9596\n",
      "Epoch 2/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9665Epoch 00001: val_loss improved from 0.10741 to 0.10522, saving model to ../data/weights/atc1_slightly_less_dropout_best_lr3.hk\n",
      "34479/34479 [==============================] - 181s - loss: 0.0846 - acc: 0.9665 - val_loss: 0.1052 - val_acc: 0.9608\n",
      "Epoch 3/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9668Epoch 00002: val_loss improved from 0.10522 to 0.10508, saving model to ../data/weights/atc1_slightly_less_dropout_best_lr3.hk\n",
      "34479/34479 [==============================] - 183s - loss: 0.0836 - acc: 0.9668 - val_loss: 0.1051 - val_acc: 0.9605\n",
      "Epoch 4/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9673Epoch 00003: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0827 - acc: 0.9673 - val_loss: 0.1067 - val_acc: 0.9599\n",
      "Epoch 5/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9681Epoch 00004: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0809 - acc: 0.9681 - val_loss: 0.1074 - val_acc: 0.9600\n",
      "Epoch 6/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9680Epoch 00005: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0804 - acc: 0.9680 - val_loss: 0.1100 - val_acc: 0.9603\n",
      "Epoch 7/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9687Epoch 00006: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0791 - acc: 0.9687 - val_loss: 0.1080 - val_acc: 0.9601\n",
      "Epoch 8/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9693Epoch 00007: val_loss did not improve\n",
      "34479/34479 [==============================] - 180s - loss: 0.0765 - acc: 0.9693 - val_loss: 0.1081 - val_acc: 0.9597\n",
      "Epoch 9/20\n",
      "34432/34479 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9697Epoch 00008: val_loss did not improve\n",
      "34479/34479 [==============================] - 181s - loss: 0.0760 - acc: 0.9698 - val_loss: 0.1102 - val_acc: 0.9596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f682aa43e10>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "vgg.model.fit_generator(traingen, samples_per_epoch=traingen.nb_sample, nb_epoch=20,\n",
    "                        validation_data=valgen, nb_val_samples=valgen.nb_sample,\n",
    "                        callbacks=[history, saver, earlystop])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
