{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "#virtualenv -p python3 venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import cPickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "base_module_path = os.path.abspath(os.path.join('..'))\n",
    "if base_module_path not in sys.path:\n",
    "    sys.path.append(base_module_path)\n",
    "import zill as z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__base_path = '../data/'\n",
    "\n",
    "def make_dtrain(outliers=False):\n",
    "    prop = pd.read_csv(__base_path + 'properties_2016.csv')\n",
    "    logerrors = pd.read_csv(__base_path + 'train_2016_v2.csv')\n",
    "\n",
    "    prop = z.data._clean_encode_props(prop)\n",
    "            \n",
    "    merged = logerrors.merge(prop, how='left', on='parcelid')\n",
    "    if outliers:\n",
    "        print('Removing outliers')\n",
    "        merged = merged[ merged.logerror > outliers[0] ]\n",
    "        merged = merged[ merged.logerror < outliers[1] ]\n",
    "    else: print('Keeping outliers')\n",
    "\n",
    "    x_train = merged.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "    y_train = merged['logerror']\n",
    "\n",
    "    # return y_mean also\n",
    "    y_mean = np.mean(y_train)\n",
    "\n",
    "    #del merged, prop, logerrors, test; gc.collect()\n",
    "\n",
    "    return xgb.DMatrix(x_train, label=y_train), y_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing outliers\n"
     ]
    }
   ],
   "source": [
    "dtrain, y_mean = make_dtrain((-0.39, 0.41)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88458,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.get_label().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of changes\n",
    "\n",
    "#### Parameter tuning\n",
    "1. Biggest change: from good initialisation, aka setting base_score to the mean\n",
    "2. Next biggest: using a lower learning rate to hit about 200-250 boost rounds\n",
    "    * note: it seems to take longer to do each boost round at lower eta also\n",
    "3. After: max_depth at certain times helped, as did the sampling, as did alpha\n",
    "\n",
    "For future: best thing probably is to do the first 2 by hand, then use sklearn GridSearchCV. \n",
    "\n",
    "#### Other stuff\n",
    "Removed outliers (self defined as meanÂ±2.5*std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versions, changes, results\n",
    "1. eval_metric=mean absolute error \n",
    "    * as per competition evaluation\n",
    "2. lower eta 0.3->0.03 \n",
    "    * model was reaching min at 19 steps. \n",
    "    * results: step224, 0.068668\n",
    "3. subsample 1->0.5\n",
    "    * equivalent to dropout? should help prevent overfit\n",
    "    * results: step215, 0.068683\n",
    "4. gamma 0->0.2\n",
    "    * lowers model complexity by only splitting node when loss reduction is >gamma (aka regularization)\n",
    "    * again should help overfitting\n",
    "    * results: step211, 0.068696\n",
    "5. max_depth 6->5\n",
    "    * other overfitting stuff doesn't seem to be changing much. lower complexity\n",
    "    * results: step228, test0.068570 train0.067371\n",
    "6. alpha 0->1\n",
    "    * cranking it see if we can do something about this overfitting\n",
    "    * results: step227, test0.068465 train0.067324\n",
    "7. USE PARAMS 2 everything-(except eta)>default, base_score->y_mean\n",
    "    * should just cut out first ~50 iterations\n",
    "    * results: step23, test0.068146, train0.067734\n",
    "        * NOTE: not only did it make the train MUCH faster, but also improved test score beyond previous best!\n",
    "        * initialisation fucking matters u prick\n",
    "8. reverse step7 apart from base_score->y_mean\n",
    "    * adding back regularization etc\n",
    "    * is it worth redoing this?\n",
    "    * results: step38 test0.068045, train0.067750\n",
    "9. colsample_bytree 1->0.8\n",
    "    * again adding more randomness (aka dropout)\n",
    "    * results: step38 test0.068031 train0.067757\n",
    "10. subsample 0.5->0.8\n",
    "    * yet more dropout (OOPS THIS WAS WRONG its less! oops)\n",
    "    * results: step45 test0.068035 train0.067637\n",
    "11. min_child_weight 1->4\n",
    "    * supposed to be one that makes a big diff (that and max depth) should have probably done earlier\n",
    "    * results: step46 test0.068021 train0.067650\n",
    "12. min_child_weight 4->6\n",
    "    * did alright last time let's crank it\n",
    "    * results: step39 test0.068021 train0.067715\n",
    "13. max_depth 5->4\n",
    "    * again supposed to make a big diff, so let's see\n",
    "    * results: step46 test0.068011 train0.067805\n",
    "14. subsample 0.8->0.5\n",
    "    * return to more dropout (im an idiot)\n",
    "    * results: step55 test0.068022 train0.067809\n",
    "15. colsample_bytree 0.8->0.5\n",
    "    * more dropout\n",
    "    * results: step59 test0.068013 train0.067808\n",
    "16. gamma 0.2->0\n",
    "    * let's see\n",
    "    * results: step59 test0.068011 train0.067791\n",
    "17. eta 0.03->0.01\n",
    "    * we're reaching plateau pretty fast, so let's lower it\n",
    "    * results: step205 test0.067984 train0.067746\n",
    "18. max_depth 4->5\n",
    "    * not underfitting but try increasing complexity\n",
    "    * results: step200 test0.067959 train0.067584\n",
    "19. min_child_weight 6->5\n",
    "    * not underfitting but try increasing complexity\n",
    "    * results: step198 test0.067964 train0.067586\n",
    "20. min_child_weight 5->3\n",
    "    * not underfitting but try increasing complexity\n",
    "    * results: step200 test0.067968 train0.067572\n",
    "21. max_depth 5->6\n",
    "    * not underfitting but try increasing complexity\n",
    "    * results: step175 test0.067958 train0.067416\n",
    "22. reverse to v18\n",
    "    * it was better\n",
    "    * results: step200 test0.067959 train0.067584\t\n",
    "23. eta 0.01->0.005 (num_boost_rounds->600)\n",
    "    * lower eta seems to make a big difference, so lets see\n",
    "    * increasing num_boost_rounds as its likely to take double the time\n",
    "    * results: step304 test0.067967 train0.067674\n",
    "24. revert to v22\n",
    "    * it was better\n",
    "    * results: step200 test0.067959 train0.067584\n",
    "25. remove outliers (more than 2.5*std from the mean)\n",
    "    * results: step304, effective train0.067168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eval_metric':'mae', #mean absolute error\n",
    "    'eta':0.01, #learning rate, default 0.3\n",
    "    'subsample':0.5,\n",
    "    'colsample_bytree':0.5,\n",
    "    'gamma':0,\n",
    "    'max_depth':5,\n",
    "    'min_child_weight':6,\n",
    "    'alpha':1,\n",
    "    'base_score':y_mean,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.0530246\ttest-mae:0.0530262\n",
      "[10]\ttrain-mae:0.0529434\ttest-mae:0.0529586\n",
      "[20]\ttrain-mae:0.052868\ttest-mae:0.0528968\n",
      "[30]\ttrain-mae:0.052801\ttest-mae:0.0528436\n",
      "[40]\ttrain-mae:0.0527398\ttest-mae:0.0527964\n",
      "[50]\ttrain-mae:0.0526846\ttest-mae:0.0527532\n",
      "[60]\ttrain-mae:0.052635\ttest-mae:0.0527176\n",
      "[70]\ttrain-mae:0.0525892\ttest-mae:0.0526844\n",
      "[80]\ttrain-mae:0.0525476\ttest-mae:0.0526536\n",
      "[90]\ttrain-mae:0.052509\ttest-mae:0.0526286\n",
      "[100]\ttrain-mae:0.0524716\ttest-mae:0.0526032\n",
      "[110]\ttrain-mae:0.0524392\ttest-mae:0.052583\n",
      "[120]\ttrain-mae:0.0524068\ttest-mae:0.052562\n",
      "[130]\ttrain-mae:0.0523784\ttest-mae:0.052544\n",
      "[140]\ttrain-mae:0.0523514\ttest-mae:0.0525276\n",
      "[150]\ttrain-mae:0.0523264\ttest-mae:0.0525134\n",
      "[160]\ttrain-mae:0.0523008\ttest-mae:0.052498\n",
      "[170]\ttrain-mae:0.0522782\ttest-mae:0.0524876\n",
      "[180]\ttrain-mae:0.052254\ttest-mae:0.052475\n",
      "[190]\ttrain-mae:0.0522326\ttest-mae:0.0524644\n",
      "[200]\ttrain-mae:0.0522106\ttest-mae:0.0524536\n",
      "[210]\ttrain-mae:0.0521906\ttest-mae:0.0524456\n",
      "[220]\ttrain-mae:0.0521714\ttest-mae:0.0524358\n",
      "[230]\ttrain-mae:0.0521536\ttest-mae:0.0524286\n",
      "[240]\ttrain-mae:0.0521354\ttest-mae:0.0524202\n",
      "[250]\ttrain-mae:0.0521176\ttest-mae:0.052413\n",
      "[260]\ttrain-mae:0.0521018\ttest-mae:0.0524078\n",
      "[270]\ttrain-mae:0.0520854\ttest-mae:0.0524006\n",
      "[280]\ttrain-mae:0.0520688\ttest-mae:0.052395\n",
      "[290]\ttrain-mae:0.0520544\ttest-mae:0.0523892\n",
      "[300]\ttrain-mae:0.052039\ttest-mae:0.0523844\n"
     ]
    }
   ],
   "source": [
    "cv_result = xgb.cv(params, dtrain, \n",
    "                   nfold=5, \n",
    "                   num_boost_round=305, \n",
    "                   early_stopping_rounds=50, \n",
    "                   verbose_eval=10, \n",
    "                   show_stdv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-mae-mean</th>\n",
       "      <th>test-mae-std</th>\n",
       "      <th>train-mae-mean</th>\n",
       "      <th>train-mae-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.052384</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.052039</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.052384</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.052038</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.052384</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.052036</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.052383</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.052035</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0.052383</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.052033</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
       "300       0.052384      0.000394        0.052039       0.000107\n",
       "301       0.052384      0.000394        0.052038       0.000107\n",
       "302       0.052384      0.000394        0.052036       0.000107\n",
       "303       0.052383      0.000394        0.052035       0.000107\n",
       "304       0.052383      0.000394        0.052033       0.000107"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following to try to assess the outlier-less model - since it's obviously gonna have better CV scores, let's try to predict on all the data and see what we get (and compare it to the train CV score for the other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mae:0.053025\n",
      "[1]\teval-mae:0.053016\n",
      "[2]\teval-mae:0.053007\n",
      "[3]\teval-mae:0.052998\n",
      "[4]\teval-mae:0.052988\n",
      "[5]\teval-mae:0.052979\n",
      "[6]\teval-mae:0.052968\n",
      "[7]\teval-mae:0.052961\n",
      "[8]\teval-mae:0.052952\n",
      "[9]\teval-mae:0.052944\n",
      "[10]\teval-mae:0.052937\n",
      "[11]\teval-mae:0.05293\n",
      "[12]\teval-mae:0.052923\n",
      "[13]\teval-mae:0.052917\n",
      "[14]\teval-mae:0.052911\n",
      "[15]\teval-mae:0.052902\n",
      "[16]\teval-mae:0.052895\n",
      "[17]\teval-mae:0.052889\n",
      "[18]\teval-mae:0.052883\n",
      "[19]\teval-mae:0.052875\n",
      "[20]\teval-mae:0.052869\n",
      "[21]\teval-mae:0.052863\n",
      "[22]\teval-mae:0.052858\n",
      "[23]\teval-mae:0.05285\n",
      "[24]\teval-mae:0.052843\n",
      "[25]\teval-mae:0.052834\n",
      "[26]\teval-mae:0.052827\n",
      "[27]\teval-mae:0.052818\n",
      "[28]\teval-mae:0.052813\n",
      "[29]\teval-mae:0.052806\n",
      "[30]\teval-mae:0.052797\n",
      "[31]\teval-mae:0.052792\n",
      "[32]\teval-mae:0.052784\n",
      "[33]\teval-mae:0.052777\n",
      "[34]\teval-mae:0.052771\n",
      "[35]\teval-mae:0.052765\n",
      "[36]\teval-mae:0.052759\n",
      "[37]\teval-mae:0.052754\n",
      "[38]\teval-mae:0.052747\n",
      "[39]\teval-mae:0.052741\n",
      "[40]\teval-mae:0.052736\n",
      "[41]\teval-mae:0.052731\n",
      "[42]\teval-mae:0.052726\n",
      "[43]\teval-mae:0.05272\n",
      "[44]\teval-mae:0.052714\n",
      "[45]\teval-mae:0.052708\n",
      "[46]\teval-mae:0.052702\n",
      "[47]\teval-mae:0.052697\n",
      "[48]\teval-mae:0.052692\n",
      "[49]\teval-mae:0.052687\n",
      "[50]\teval-mae:0.052683\n",
      "[51]\teval-mae:0.052677\n",
      "[52]\teval-mae:0.052674\n",
      "[53]\teval-mae:0.05267\n",
      "[54]\teval-mae:0.052666\n",
      "[55]\teval-mae:0.052661\n",
      "[56]\teval-mae:0.052657\n",
      "[57]\teval-mae:0.052653\n",
      "[58]\teval-mae:0.052649\n",
      "[59]\teval-mae:0.052645\n",
      "[60]\teval-mae:0.052641\n",
      "[61]\teval-mae:0.052636\n",
      "[62]\teval-mae:0.052632\n",
      "[63]\teval-mae:0.052628\n",
      "[64]\teval-mae:0.052624\n",
      "[65]\teval-mae:0.05262\n",
      "[66]\teval-mae:0.052615\n",
      "[67]\teval-mae:0.052609\n",
      "[68]\teval-mae:0.052603\n",
      "[69]\teval-mae:0.052599\n",
      "[70]\teval-mae:0.052596\n",
      "[71]\teval-mae:0.052591\n",
      "[72]\teval-mae:0.052588\n",
      "[73]\teval-mae:0.052583\n",
      "[74]\teval-mae:0.052579\n",
      "[75]\teval-mae:0.052574\n",
      "[76]\teval-mae:0.052569\n",
      "[77]\teval-mae:0.052565\n",
      "[78]\teval-mae:0.052561\n",
      "[79]\teval-mae:0.052557\n",
      "[80]\teval-mae:0.052554\n",
      "[81]\teval-mae:0.052551\n",
      "[82]\teval-mae:0.052547\n",
      "[83]\teval-mae:0.052542\n",
      "[84]\teval-mae:0.052538\n",
      "[85]\teval-mae:0.052534\n",
      "[86]\teval-mae:0.05253\n",
      "[87]\teval-mae:0.052526\n",
      "[88]\teval-mae:0.052521\n",
      "[89]\teval-mae:0.052518\n",
      "[90]\teval-mae:0.052513\n",
      "[91]\teval-mae:0.052509\n",
      "[92]\teval-mae:0.052506\n",
      "[93]\teval-mae:0.052504\n",
      "[94]\teval-mae:0.052501\n",
      "[95]\teval-mae:0.052498\n",
      "[96]\teval-mae:0.052494\n",
      "[97]\teval-mae:0.052491\n",
      "[98]\teval-mae:0.052487\n",
      "[99]\teval-mae:0.052484\n",
      "[100]\teval-mae:0.052481\n",
      "[101]\teval-mae:0.052478\n",
      "[102]\teval-mae:0.052475\n",
      "[103]\teval-mae:0.052472\n",
      "[104]\teval-mae:0.052468\n",
      "[105]\teval-mae:0.052465\n",
      "[106]\teval-mae:0.052462\n",
      "[107]\teval-mae:0.052459\n",
      "[108]\teval-mae:0.052457\n",
      "[109]\teval-mae:0.052454\n",
      "[110]\teval-mae:0.052452\n",
      "[111]\teval-mae:0.052447\n",
      "[112]\teval-mae:0.052444\n",
      "[113]\teval-mae:0.052441\n",
      "[114]\teval-mae:0.052438\n",
      "[115]\teval-mae:0.052436\n",
      "[116]\teval-mae:0.052433\n",
      "[117]\teval-mae:0.05243\n",
      "[118]\teval-mae:0.052428\n",
      "[119]\teval-mae:0.052425\n",
      "[120]\teval-mae:0.052422\n",
      "[121]\teval-mae:0.052418\n",
      "[122]\teval-mae:0.052415\n",
      "[123]\teval-mae:0.052412\n",
      "[124]\teval-mae:0.05241\n",
      "[125]\teval-mae:0.052406\n",
      "[126]\teval-mae:0.052403\n",
      "[127]\teval-mae:0.052401\n",
      "[128]\teval-mae:0.052399\n",
      "[129]\teval-mae:0.052395\n",
      "[130]\teval-mae:0.052393\n",
      "[131]\teval-mae:0.05239\n",
      "[132]\teval-mae:0.052387\n",
      "[133]\teval-mae:0.052385\n",
      "[134]\teval-mae:0.052382\n",
      "[135]\teval-mae:0.052379\n",
      "[136]\teval-mae:0.052377\n",
      "[137]\teval-mae:0.052374\n",
      "[138]\teval-mae:0.052371\n",
      "[139]\teval-mae:0.052368\n",
      "[140]\teval-mae:0.052365\n",
      "[141]\teval-mae:0.052363\n",
      "[142]\teval-mae:0.05236\n",
      "[143]\teval-mae:0.052357\n",
      "[144]\teval-mae:0.052354\n",
      "[145]\teval-mae:0.052352\n",
      "[146]\teval-mae:0.052351\n",
      "[147]\teval-mae:0.052348\n",
      "[148]\teval-mae:0.052346\n",
      "[149]\teval-mae:0.052344\n",
      "[150]\teval-mae:0.052341\n",
      "[151]\teval-mae:0.052339\n",
      "[152]\teval-mae:0.052337\n",
      "[153]\teval-mae:0.052334\n",
      "[154]\teval-mae:0.052332\n",
      "[155]\teval-mae:0.05233\n",
      "[156]\teval-mae:0.052327\n",
      "[157]\teval-mae:0.052325\n",
      "[158]\teval-mae:0.052323\n",
      "[159]\teval-mae:0.05232\n",
      "[160]\teval-mae:0.052318\n",
      "[161]\teval-mae:0.052316\n",
      "[162]\teval-mae:0.052313\n",
      "[163]\teval-mae:0.052312\n",
      "[164]\teval-mae:0.05231\n",
      "[165]\teval-mae:0.052307\n",
      "[166]\teval-mae:0.052305\n",
      "[167]\teval-mae:0.052302\n",
      "[168]\teval-mae:0.052301\n",
      "[169]\teval-mae:0.052299\n",
      "[170]\teval-mae:0.052297\n",
      "[171]\teval-mae:0.052294\n",
      "[172]\teval-mae:0.052292\n",
      "[173]\teval-mae:0.05229\n",
      "[174]\teval-mae:0.052289\n",
      "[175]\teval-mae:0.052287\n",
      "[176]\teval-mae:0.052285\n",
      "[177]\teval-mae:0.052283\n",
      "[178]\teval-mae:0.052281\n",
      "[179]\teval-mae:0.052279\n",
      "[180]\teval-mae:0.052277\n",
      "[181]\teval-mae:0.052274\n",
      "[182]\teval-mae:0.052272\n",
      "[183]\teval-mae:0.052269\n",
      "[184]\teval-mae:0.052267\n",
      "[185]\teval-mae:0.052265\n",
      "[186]\teval-mae:0.052264\n",
      "[187]\teval-mae:0.052261\n",
      "[188]\teval-mae:0.052259\n",
      "[189]\teval-mae:0.052258\n",
      "[190]\teval-mae:0.052257\n",
      "[191]\teval-mae:0.052254\n",
      "[192]\teval-mae:0.052252\n",
      "[193]\teval-mae:0.05225\n",
      "[194]\teval-mae:0.052248\n",
      "[195]\teval-mae:0.052246\n",
      "[196]\teval-mae:0.052243\n",
      "[197]\teval-mae:0.052242\n",
      "[198]\teval-mae:0.05224\n",
      "[199]\teval-mae:0.052238\n",
      "[200]\teval-mae:0.052237\n",
      "[201]\teval-mae:0.052235\n",
      "[202]\teval-mae:0.052233\n",
      "[203]\teval-mae:0.052231\n",
      "[204]\teval-mae:0.052229\n",
      "[205]\teval-mae:0.052227\n",
      "[206]\teval-mae:0.052225\n",
      "[207]\teval-mae:0.052224\n",
      "[208]\teval-mae:0.052223\n",
      "[209]\teval-mae:0.052221\n",
      "[210]\teval-mae:0.05222\n",
      "[211]\teval-mae:0.052218\n",
      "[212]\teval-mae:0.052216\n",
      "[213]\teval-mae:0.052214\n",
      "[214]\teval-mae:0.052213\n",
      "[215]\teval-mae:0.052211\n",
      "[216]\teval-mae:0.052209\n",
      "[217]\teval-mae:0.052207\n",
      "[218]\teval-mae:0.052206\n",
      "[219]\teval-mae:0.052205\n",
      "[220]\teval-mae:0.052203\n",
      "[221]\teval-mae:0.052202\n",
      "[222]\teval-mae:0.052201\n",
      "[223]\teval-mae:0.0522\n",
      "[224]\teval-mae:0.052197\n",
      "[225]\teval-mae:0.052195\n",
      "[226]\teval-mae:0.052194\n",
      "[227]\teval-mae:0.052192\n",
      "[228]\teval-mae:0.052189\n",
      "[229]\teval-mae:0.052188\n",
      "[230]\teval-mae:0.052186\n",
      "[231]\teval-mae:0.052185\n",
      "[232]\teval-mae:0.052182\n",
      "[233]\teval-mae:0.052179\n",
      "[234]\teval-mae:0.052178\n",
      "[235]\teval-mae:0.052176\n",
      "[236]\teval-mae:0.052175\n",
      "[237]\teval-mae:0.052173\n",
      "[238]\teval-mae:0.052171\n",
      "[239]\teval-mae:0.05217\n",
      "[240]\teval-mae:0.052168\n",
      "[241]\teval-mae:0.052166\n",
      "[242]\teval-mae:0.052164\n",
      "[243]\teval-mae:0.052163\n",
      "[244]\teval-mae:0.052161\n",
      "[245]\teval-mae:0.052159\n",
      "[246]\teval-mae:0.052157\n",
      "[247]\teval-mae:0.052156\n",
      "[248]\teval-mae:0.052154\n",
      "[249]\teval-mae:0.052152\n",
      "[250]\teval-mae:0.05215\n",
      "[251]\teval-mae:0.052148\n",
      "[252]\teval-mae:0.052147\n",
      "[253]\teval-mae:0.052146\n",
      "[254]\teval-mae:0.052143\n",
      "[255]\teval-mae:0.052142\n",
      "[256]\teval-mae:0.052141\n",
      "[257]\teval-mae:0.05214\n",
      "[258]\teval-mae:0.052138\n",
      "[259]\teval-mae:0.052136\n",
      "[260]\teval-mae:0.052133\n",
      "[261]\teval-mae:0.052132\n",
      "[262]\teval-mae:0.052131\n",
      "[263]\teval-mae:0.05213\n",
      "[264]\teval-mae:0.052129\n",
      "[265]\teval-mae:0.052128\n",
      "[266]\teval-mae:0.052126\n",
      "[267]\teval-mae:0.052124\n",
      "[268]\teval-mae:0.052123\n",
      "[269]\teval-mae:0.052121\n",
      "[270]\teval-mae:0.05212\n",
      "[271]\teval-mae:0.052118\n",
      "[272]\teval-mae:0.052116\n",
      "[273]\teval-mae:0.052114\n",
      "[274]\teval-mae:0.052112\n",
      "[275]\teval-mae:0.052112\n",
      "[276]\teval-mae:0.05211\n",
      "[277]\teval-mae:0.052108\n",
      "[278]\teval-mae:0.052107\n",
      "[279]\teval-mae:0.052106\n",
      "[280]\teval-mae:0.052105\n",
      "[281]\teval-mae:0.052103\n",
      "[282]\teval-mae:0.052102\n",
      "[283]\teval-mae:0.052101\n",
      "[284]\teval-mae:0.0521\n",
      "[285]\teval-mae:0.052098\n",
      "[286]\teval-mae:0.052096\n",
      "[287]\teval-mae:0.052095\n",
      "[288]\teval-mae:0.052094\n",
      "[289]\teval-mae:0.052093\n",
      "[290]\teval-mae:0.052091\n",
      "[291]\teval-mae:0.05209\n",
      "[292]\teval-mae:0.052089\n",
      "[293]\teval-mae:0.052088\n",
      "[294]\teval-mae:0.052086\n",
      "[295]\teval-mae:0.052085\n",
      "[296]\teval-mae:0.052084\n",
      "[297]\teval-mae:0.052084\n",
      "[298]\teval-mae:0.052082\n",
      "[299]\teval-mae:0.052081\n",
      "[300]\teval-mae:0.05208\n",
      "[301]\teval-mae:0.052078\n",
      "[302]\teval-mae:0.052077\n",
      "[303]\teval-mae:0.052076\n",
      "[304]\teval-mae:0.052075\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(dict(params), dtrain, num_boost_round=len(cv_result), \n",
    "                  evals=[(dtrain, 'eval')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn \n",
    "def assess_outlier_model(model):\n",
    "    print('Getting all data')\n",
    "\n",
    "    dtest, y_mean = z.data.make_dtrain()\n",
    "    y_test = dtest.get_label()\n",
    "    print(y_test.shape)\n",
    "    dtest.set_label([])\n",
    "    print('Predicting...')\n",
    "    pred = model.predict(dtest)\n",
    "    y_pred = []\n",
    "    for i, predict in enumerate(pred):\n",
    "        y_pred.append(round(predict, 4))\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return sklearn.metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all data\n",
      "(90275,)\n",
      "Predicting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.067168124069345858"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess_outlier_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2828: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing results to disk ...\n",
      "\n",
      "Written results to disk.\n"
     ]
    }
   ],
   "source": [
    "z.data.make_submission_with_model(model)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
